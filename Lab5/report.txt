The file was generated by sampling some random data (probably /dev/urandom/) and generating a 1M chunk, then appending that chunk to a file ten times. I know this because the md5sums of the file split into 1M parts are all equal, which means that the original chunk's size is at most 1M, but the 512K file shows two patterns - the first and second half of the original 1M file, since 512K (or 2^19) is half of 1M (or 2^20). The 1MB split is basically noise, since it doesn't split evenly and therefore reads ~10/11ths of the original 1M file, making all the md4sums different.
